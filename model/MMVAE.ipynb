{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch.utils.data\n",
    "from scipy import misc\n",
    "from torch import optim\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm.auto import tqdm, trange\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from collections import namedtuple\n",
    "import torchvision.models as models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from laserembeddings import Laser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable gpu device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 8888\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join('../', 'data', 'TRAINING','images')\n",
    "trial_dir = os.path.join('../', 'data', 'Users', 'fersiniel', 'Desktop', 'MAMI - TO LABEL/TRIAL DATASET', 'images')\n",
    "\n",
    "# load training label\n",
    "train_df = pd.read_csv('../data/TRAINING/training.csv', sep='\\t')\n",
    "# load trial label\n",
    "trial_df = pd.read_csv('../data/Users/fersiniel/Desktop/MAMI - TO LABEL/TRIAL DATASET/trial.csv', sep='\\t')\n",
    "# load test label\n",
    "test_df = pd.read_csv('../data/test/Test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misogynous</th>\n",
       "      <th>shaming</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>objectification</th>\n",
       "      <th>violence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.127400</td>\n",
       "      <td>0.28100</td>\n",
       "      <td>0.220200</td>\n",
       "      <td>0.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500025</td>\n",
       "      <td>0.333437</td>\n",
       "      <td>0.44951</td>\n",
       "      <td>0.414402</td>\n",
       "      <td>0.293644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         misogynous       shaming   stereotype  objectification      violence\n",
       "count  10000.000000  10000.000000  10000.00000     10000.000000  10000.000000\n",
       "mean       0.500000      0.127400      0.28100         0.220200      0.095300\n",
       "std        0.500025      0.333437      0.44951         0.414402      0.293644\n",
       "min        0.000000      0.000000      0.00000         0.000000      0.000000\n",
       "25%        0.000000      0.000000      0.00000         0.000000      0.000000\n",
       "50%        0.500000      0.000000      0.00000         0.000000      0.000000\n",
       "75%        1.000000      0.000000      1.00000         0.000000      0.000000\n",
       "max        1.000000      1.000000      1.00000         1.000000      1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>misogynous</th>\n",
       "      <th>shaming</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>objectification</th>\n",
       "      <th>violence</th>\n",
       "      <th>Text Transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Milk Milk.zip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>BREAKING NEWS: Russia releases photo of DONALD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>MAN SEEKING WOMAN Ignad 18 O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10006.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Me explaining the deep lore of. J.R.R. Tolkein...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  misogynous  shaming  stereotype  objectification  violence  \\\n",
       "0      1.jpg           0        0           0                0         0   \n",
       "1     10.jpg           1        0           0                0         1   \n",
       "2   1000.jpg           0        0           0                0         0   \n",
       "3  10000.jpg           0        0           0                0         0   \n",
       "4  10006.jpg           0        0           0                0         0   \n",
       "\n",
       "                                  Text Transcription  \n",
       "0                                      Milk Milk.zip  \n",
       "1  ROSES ARE RED, VIOLETS ARE BLUE IF YOU DON'T S...  \n",
       "2  BREAKING NEWS: Russia releases photo of DONALD...  \n",
       "3                       MAN SEEKING WOMAN Ignad 18 O  \n",
       "4  Me explaining the deep lore of. J.R.R. Tolkein...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define image transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_size = 128\n",
    "# pretrained_size = 256\n",
    "pretrained_size = 224\n",
    "pretrained_means = [0.485, 0.456, 0.406]\n",
    "pretrained_stds= [0.229, 0.224, 0.225]\n",
    "\n",
    "# train_transforms = transforms.Compose([\n",
    "#                            transforms.Resize(pretrained_size),\n",
    "#                            transforms.RandomRotation(5),\n",
    "#                            transforms.RandomHorizontalFlip(0.5),\n",
    "#                            transforms.RandomCrop(pretrained_size, padding = 10),\n",
    "#                            transforms.ToTensor(),\n",
    "#                            transforms.Normalize(mean = pretrained_means, \n",
    "#                                                 std = pretrained_stds)\n",
    "#                        ])\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "                           transforms.ToTensor()\n",
    "                       ])\n",
    "\n",
    "# trial_transforms = transforms.Compose([\n",
    "#                            transforms.Resize(pretrained_size),\n",
    "#                            transforms.CenterCrop(pretrained_size),\n",
    "#                            transforms.ToTensor(),\n",
    "#                            transforms.Normalize(mean = pretrained_means, \n",
    "#                                                 std = pretrained_stds)\n",
    "#                        ])\n",
    "\n",
    "trial_transforms = train_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ResNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, config, output_dim):\n",
    "        super().__init__()\n",
    "                \n",
    "        block, n_blocks, channels = config\n",
    "        self.in_channels = channels[0]\n",
    "            \n",
    "        assert len(n_blocks) == len(channels) == 4\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size = 7, stride = 2, padding = 3, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
    "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride = 2)\n",
    "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride = 2)\n",
    "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride = 2)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
    "        \n",
    "    def get_resnet_layer(self, block, n_blocks, channels, stride = 1):\n",
    "    \n",
    "        layers = []\n",
    "        \n",
    "        if self.in_channels != block.expansion * channels:\n",
    "            downsample = True\n",
    "        else:\n",
    "            downsample = False\n",
    "        \n",
    "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
    "        \n",
    "        for i in range(1, n_blocks):\n",
    "            layers.append(block(block.expansion * channels, channels))\n",
    "\n",
    "        self.in_channels = block.expansion * channels\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        h = x.view(x.shape[0], -1)\n",
    "        x = self.fc(h)\n",
    "        \n",
    "        return x, h\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    \n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, \n",
    "                               stride = stride, padding = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n",
    "                               stride = 1, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        if downsample:\n",
    "            conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n",
    "                             stride = stride, bias = False)\n",
    "            bn = nn.BatchNorm2d(out_channels)\n",
    "            downsample = nn.Sequential(conv, bn)\n",
    "        else:\n",
    "            downsample = None\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        i = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            i = self.downsample(i)\n",
    "                        \n",
    "        x += i\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = False):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 1, \n",
    "                               stride = 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, \n",
    "                               stride = stride, padding = 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(out_channels, self.expansion * out_channels, kernel_size = 1,\n",
    "                               stride = 1, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * out_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        if downsample:\n",
    "            conv = nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = 1, \n",
    "                             stride = stride, bias = False)\n",
    "            bn = nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            downsample = nn.Sequential(conv, bn)\n",
    "        else:\n",
    "            downsample = None\n",
    "            \n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        i = x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "                \n",
    "        if self.downsample is not None:\n",
    "            i = self.downsample(i)\n",
    "            \n",
    "        x += i\n",
    "        x = self.relu(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad_false(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Bottleneck' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b0b62bf2e160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mResNetConfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ResNetConfig'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'block'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_blocks'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'channels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m resnet50_config = ResNetConfig(block = Bottleneck,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                \u001b[0mn_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                channels = [64, 128, 256, 512])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Bottleneck' is not defined"
     ]
    }
   ],
   "source": [
    "# Bert pretrained model\n",
    "\n",
    "bert_pretrained = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        \n",
    "set_parameter_requires_grad_false(bert_pretrained)\n",
    "\n",
    "bert_pretrained.cuda()\n",
    "bert_pretrained.eval()\n",
    "\n",
    "# ResNet pretrained model\n",
    "    \n",
    "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])\n",
    "resnet50_config = ResNetConfig(block = Bottleneck,\n",
    "                               n_blocks = [3, 4, 6, 3],\n",
    "                               channels = [64, 128, 256, 512])\n",
    "pretrained_model = models.resnet50(pretrained = True)\n",
    "        \n",
    "resnet_pretrained = ResNet(resnet50_config, 1000)\n",
    "resnet_pretrained.load_state_dict(pretrained_model.state_dict())\n",
    "        \n",
    "set_parameter_requires_grad_false(resnet_pretrained)\n",
    "\n",
    "resnet_pretrained.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip pretrained model for image encoding\n",
    "\n",
    "clip_pretrained = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "# Laser model for text encoding\n",
    "\n",
    "laser_model = Laser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = laser_model.embed_sentences(\"Hello World!\", lang='en')\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-define Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMIDataset(Dataset):\n",
    "    \"\"\"MAMI dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file, sep='\\t')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 0])\n",
    "        meme = Image.open(img_name)#.convert(\"RGB\")   # convert to RGB is important\n",
    "        meme = torch.Tensor(clip_pretrained.encode(meme))\n",
    "        labels = self.df.iloc[idx, 1:-1]   # multi-labels\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.astype('long')\n",
    "        \n",
    "        text = self.df.iloc[idx, -1]   # Text transcription\n",
    "        text = torch.Tensor(laser_model.embed_sentences(text, lang='en'))\n",
    "#         text = \"[CLS] \" + text + \" [SEP]\"   # Add special tokens\n",
    "        \n",
    "#         tokenized_text = tokenizer.tokenize(text)\n",
    "#         # Map the token strings to their vocabulary indeces.\n",
    "#         indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#         segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "#         tokens_tensor = torch.tensor([indexed_tokens])\n",
    "#         segments_tensor = torch.tensor([segments_ids])\n",
    "\n",
    "#         text_ids = tokenizer(text, return_tensors=\"pt\", padding='max_length', max_length=512, truncation=True)\n",
    "        \n",
    "        sample = {'meme': meme, 'labels': labels, 'text': text}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['meme'] = self.transform(meme)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAMITestset(Dataset):\n",
    "    \"\"\"MAMI dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file, sep='\\t')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.df.iloc[idx, 0])\n",
    "        meme = Image.open(img_name)#.convert(\"RGB\")   # convert to RGB is important\n",
    "        meme = torch.Tensor(clip_pretrained.encode(meme))\n",
    "        \n",
    "        text = self.df.iloc[idx, -1]   # Text transcription\n",
    "        text = torch.Tensor(laser_model.embed_sentences(text, lang='en'))\n",
    "        \n",
    "#         text = \"[CLS] \" + text + \" [SEP]\"   # Add special tokens\n",
    "        \n",
    "#         tokenized_text = tokenizer.tokenize(text)\n",
    "#         # Map the token strings to their vocabulary indeces.\n",
    "#         indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "#         segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "#         tokens_tensor = torch.tensor([indexed_tokens])\n",
    "#         segments_tensor = torch.tensor([segments_ids])\n",
    "        \n",
    "        sample = {'meme': meme, 'text': text}\n",
    "\n",
    "        if self.transform:\n",
    "            sample['meme'] = self.transform(meme)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the train and trial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_root_dir = '../data/Users/fersiniel/Desktop/MAMI - TO LABEL/TRIAL DATASET/'\n",
    "# trial_data = MAMIDataset(trial_root_dir + 'trial.csv', trial_root_dir, trial_transforms)\n",
    "trial_data = MAMIDataset(trial_root_dir + 'trial.csv', trial_root_dir)\n",
    "\n",
    "train_root_dir = '../data/TRAINING/'\n",
    "# train_data = MAMIDataset(train_root_dir + 'training.csv', train_root_dir, train_transforms)\n",
    "train_data = MAMIDataset(train_root_dir + 'training.csv', train_root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_root_dir = '../data/test/'\n",
    "# train_data = MAMIDataset(train_root_dir + 'training.csv', train_root_dir, train_transforms)\n",
    "test_data = MAMITestset(test_root_dir + 'Test.csv', test_root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_RATIO = 0.9\n",
    "\n",
    "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data, \n",
    "                                           [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "# valid_data.dataset.transform = trial_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 9000\n",
      "Number of validation examples: 1000\n",
      "Number of testing examples: 100\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(trial_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "length = len(valid_data)\n",
    "num_misogynous = 0\n",
    "for i in range(length):\n",
    "    if valid_data[i]['labels'][0] == 1:\n",
    "        num_misogynous += 1\n",
    "\n",
    "print(num_misogynous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batch iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data.DataLoader(train_data, \n",
    "                                 shuffle = True, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "valid_iterator = data.DataLoader(valid_data, \n",
    "                                 batch_size = BATCH_SIZE)\n",
    "\n",
    "trial_iterator = data.DataLoader(trial_data, \n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = data.DataLoader(test_data, \n",
    "                                batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "557"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 0\n",
    "for index, row in train_df.iterrows():\n",
    "    txt = row['Text Transcription']\n",
    "    txt = \"[CLS] \" + txt + \" [SEP]\"\n",
    "    length = len(tokenizer.tokenize(txt))\n",
    "    if max_length < length:\n",
    "        max_length = length\n",
    "    \n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.5096"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average length of Text Transcription\n",
    "avg_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "txt = \"Hello, my dog is cute\"\n",
    "txt = \"[CLS] \" + txt + \" [SEP]\"\n",
    "tokenized_txt = tokenizer.tokenize(txt)\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_txt)\n",
    "segments_ids = [1] * len(tokenized_txt)\n",
    "\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensor = torch.tensor([segments_ids])\n",
    "print(segments_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "                                  )\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the text through BERT, and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor, segments_tensor)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7624e-01,  1.1550e-01,  1.1242e-01, -5.7556e-01, -2.4503e-01,\n",
       "          8.4391e-02,  5.3658e-01,  4.9711e-01, -4.0918e-01, -1.6279e-01,\n",
       "         -2.8653e-02,  5.8704e-02,  2.1708e-01, -1.1108e-02,  7.4637e-02,\n",
       "          3.4948e-01,  2.0475e-01,  1.8479e-02,  2.1207e-01, -4.7727e-02,\n",
       "         -1.3609e-01, -2.6399e-01, -2.1642e-02, -3.0580e-01,  3.2172e-01,\n",
       "         -2.0820e-01, -2.3615e-01, -1.6109e-01,  1.0481e-01, -2.9942e-01,\n",
       "         -1.0333e-01,  5.3430e-01, -1.8657e-01, -4.1273e-01,  2.6009e-02,\n",
       "          8.9587e-02,  1.0829e-01, -2.5235e-02,  2.0665e-01,  5.0784e-01,\n",
       "         -3.6589e-01,  1.5930e-01,  1.8801e-01,  2.6812e-02, -1.4873e-01,\n",
       "         -1.5996e-01, -2.4484e+00, -2.2372e-01, -4.6786e-01, -3.4599e-01,\n",
       "         -6.4978e-02,  1.4295e-01,  1.6354e-01,  1.7121e-01, -1.9327e-01,\n",
       "          3.7022e-01, -5.4412e-01,  2.0715e-01,  5.2038e-01,  1.3504e-01,\n",
       "         -2.7291e-02, -5.9998e-02, -9.9373e-02,  1.2187e-01, -2.5336e-02,\n",
       "          4.9320e-01, -5.6170e-01,  8.1200e-01, -3.2150e-01,  6.0890e-01,\n",
       "         -7.2384e-01,  1.2692e-02,  1.5561e-01, -1.7363e-01, -2.9189e-01,\n",
       "         -2.6666e-01, -7.2970e-02, -8.3492e-02, -2.1540e-01, -2.3849e-01,\n",
       "          1.6891e-02, -3.5241e-01,  7.4463e-02, -3.2028e-01,  4.9354e-01,\n",
       "          5.3577e-01, -1.0973e-01, -3.1692e-01,  8.2285e-03,  7.7993e-01,\n",
       "         -3.8841e-02, -2.4210e-01, -1.3188e-01, -6.1443e-02,  1.8719e-01,\n",
       "          1.5157e-01,  2.2129e-01,  6.1553e-02,  3.9795e-01,  8.2185e-02,\n",
       "         -5.4557e-01,  2.2111e-02,  1.4660e-01, -1.0377e-01,  1.6798e-01,\n",
       "         -1.2479e-01, -3.4161e-01, -1.2931e-01, -3.7190e-02, -2.1460e+00,\n",
       "          3.6114e-01,  2.3807e-01, -1.3247e-01, -3.3372e-01,  2.1278e-01,\n",
       "          2.8451e-01,  1.9800e-01,  6.9342e-02, -1.8462e-01, -4.3570e-01,\n",
       "          1.5788e-01,  1.5433e-01,  1.8099e-01, -1.5875e-01,  1.3119e-01,\n",
       "          5.1972e-01, -2.3349e-03, -2.4131e-01,  4.2394e-01,  2.1200e-01,\n",
       "          1.8222e-01,  6.1231e-01,  1.7458e-01, -2.0839e-02, -6.5466e-04,\n",
       "         -5.0022e-01,  4.8477e-01,  3.4151e-01, -9.1899e-02, -3.0757e-01,\n",
       "         -4.8270e-01, -3.9578e-01, -2.8264e+00,  1.2624e-01,  8.7297e-01,\n",
       "          4.5581e-01, -8.4963e-01,  5.6020e-01, -1.3979e-01,  7.1904e-01,\n",
       "          6.9633e-02, -3.0753e-01, -1.5228e-01,  3.1401e-01, -7.5921e-01,\n",
       "          4.2925e-01,  2.6339e-01, -3.2865e-01,  8.1523e-03,  4.0691e-01,\n",
       "          2.9297e-01,  3.3483e-01,  3.2106e-01,  5.2289e-02,  1.3241e-01,\n",
       "         -3.4927e-01, -1.5889e-01,  6.7754e-02, -1.7619e-01, -1.9277e-01,\n",
       "         -4.8427e-03, -1.8275e-01,  4.7482e-01,  1.2520e-01,  3.5838e-01,\n",
       "         -6.2690e-01, -1.6290e-01,  1.4096e-01,  1.5032e-01,  2.2932e-01,\n",
       "         -3.5920e-01,  4.8154e-01,  2.3448e-01, -2.4403e-01,  2.5122e-01,\n",
       "          1.0059e-01,  2.9600e-01, -5.1344e-01,  2.5889e-01, -2.2210e-01,\n",
       "         -1.4207e-01,  7.9166e-02, -1.1994e-02, -2.7736e-01,  4.1982e-02,\n",
       "          1.8882e-01,  2.9208e-01, -6.1503e-01,  3.1595e-01,  5.6018e-01,\n",
       "         -9.5873e-02,  2.0107e-01, -1.6868e-01,  1.7126e-01,  7.5815e-02,\n",
       "          3.5901e+00,  2.1255e-01,  1.5029e-02,  6.1841e-01, -1.7740e-02,\n",
       "          4.5367e-02,  1.6076e-01,  1.9663e-01, -2.5325e-01,  4.7615e-01,\n",
       "         -2.2850e-01,  1.0860e+00, -4.4635e-01, -2.9348e-01, -1.5295e-01,\n",
       "          3.3980e-02, -5.7537e-02, -3.2032e-02,  4.5337e-01, -3.0298e-01,\n",
       "          3.3804e-01,  2.2662e-01,  3.1851e-01, -1.9674e-01, -6.3473e-01,\n",
       "          1.6998e-01,  1.6466e-01, -1.7648e-01,  8.0789e-02,  4.5683e-02,\n",
       "          6.5488e-02,  5.8421e-02,  4.2670e-02, -3.0742e-02,  2.0669e-01,\n",
       "         -1.4432e-01,  4.3681e-01,  5.9786e-02, -2.6029e-01, -2.3127e-01,\n",
       "         -5.0518e-01,  1.0435e+00, -1.2091e-01, -2.4971e-01,  8.4993e-02,\n",
       "          2.9295e-01, -1.5486e-01,  1.2291e-01, -7.4184e-01, -2.9787e-01,\n",
       "         -1.1980e-01,  4.7295e-02,  1.3128e-01,  1.0508e-01, -4.7625e-01,\n",
       "         -1.1862e-01,  2.1419e-01,  5.6038e-01, -3.5879e-01, -4.4270e-01,\n",
       "          1.8804e-01, -1.1989e-01,  1.7227e-02,  5.2940e-01, -2.8251e-01,\n",
       "          2.6407e-02,  1.6196e-01, -2.8187e-01, -3.2083e+00, -4.2521e-01,\n",
       "          1.7435e-01,  4.1709e-01,  2.2784e-01, -5.4080e-01, -3.0949e-01,\n",
       "          3.9267e-01, -2.1743e-02, -2.9189e-01, -1.1681e-01, -1.9837e-01,\n",
       "         -7.2169e-02,  1.4689e-01, -4.1932e-01,  1.3957e-01, -3.4362e-01,\n",
       "          2.3228e-01,  2.1457e-01, -1.3057e-01,  5.9172e-02,  5.5401e-01,\n",
       "         -6.3272e-01, -2.6936e-01,  6.4403e-01, -3.5659e-02, -3.2368e-01,\n",
       "          1.7447e-02,  2.3592e-01, -5.9322e-02, -2.4444e-01, -9.0071e-02,\n",
       "         -7.3796e-02, -1.7223e-01, -3.0483e-01, -4.0158e+00,  5.6193e-02,\n",
       "         -3.0848e-02, -1.3590e-01,  7.7842e-02, -1.0643e-01,  4.7385e-01,\n",
       "         -6.0634e-02,  4.9041e-01,  6.1781e-02, -1.7514e-01, -3.7478e-01,\n",
       "         -4.2694e-01, -8.2553e-02, -1.1687e-01, -3.4604e-02,  1.1833e-01,\n",
       "          2.3628e-01,  5.2109e-02,  2.3280e-01,  2.1575e-01,  1.9993e-01,\n",
       "          2.9014e-01,  9.0655e-03,  1.4727e-01,  4.3309e-01, -4.3999e-02,\n",
       "         -9.5601e-02, -3.4744e-01, -1.4977e-01, -3.3791e-02, -2.1585e-01,\n",
       "          5.7591e-02,  8.3210e-02, -1.1296e-01, -4.1011e-01,  2.0870e-01,\n",
       "          1.5449e-01,  5.5197e-01,  1.5462e-01, -3.8144e-01,  6.5238e-01,\n",
       "          3.2656e-01,  5.0548e-02,  4.7506e-01,  1.5517e-01, -2.5233e-01,\n",
       "         -4.0647e-01,  1.1413e-01,  4.5286e-02, -5.8965e-01,  5.9817e-02,\n",
       "          9.4316e-01, -2.9295e-01,  9.3985e-01, -3.3613e-02, -1.7120e-02,\n",
       "          2.4561e-01,  5.4440e-01,  1.5586e-01,  7.4958e-01, -5.9065e-02,\n",
       "         -4.1819e-01, -2.2344e-01,  3.5482e-01, -2.9947e-01,  2.0406e-01,\n",
       "         -3.7246e-01, -3.9314e-02,  1.2546e-01, -2.4329e-01,  7.7731e-02,\n",
       "          1.1129e-01, -5.8961e-01,  2.5219e-02, -1.2207e-02, -3.8574e-01,\n",
       "          7.7063e-02, -6.3751e-02,  1.8099e-01, -4.7636e-01, -1.2697e-01,\n",
       "          3.1721e-01,  6.7667e-01, -2.6372e-01,  5.3201e-01, -1.3893e-01,\n",
       "         -4.1748e-02, -2.4655e-01,  3.6538e-01, -2.0222e-01,  4.2030e-01,\n",
       "          8.8757e-02,  7.8413e-02, -3.2370e-01,  5.6662e-01,  6.3215e-01,\n",
       "         -8.0259e-01,  6.9987e-01, -3.8572e-01,  4.6252e-02, -5.2781e-01,\n",
       "         -6.1187e-01,  7.6017e-02, -1.7971e-01, -1.4585e-01, -6.1393e-01,\n",
       "          5.8375e-02,  6.1459e-01,  3.2983e-01,  2.3522e-01,  1.9385e-01,\n",
       "         -4.3783e-01,  1.4180e-01,  6.9767e-01, -2.1719e-01, -4.0905e-01,\n",
       "          3.2053e-01,  1.3304e-01,  1.4721e-01,  1.3865e-01,  3.0314e-01,\n",
       "         -1.5515e-01,  1.2555e-01,  1.7551e-01,  5.4261e-01,  3.3466e-01,\n",
       "         -1.0954e-02, -6.1516e-01, -8.7878e-02, -1.3672e-01, -4.5312e-01,\n",
       "         -2.5234e-01, -4.0823e-01,  2.0254e-01, -5.9867e-02, -2.0970e-01,\n",
       "          7.7189e-01,  4.0122e-01,  1.7754e-01,  7.4776e-02, -2.3066e-01,\n",
       "         -1.1238e-01,  2.2088e-01, -1.5820e-01,  6.3689e-01,  1.1074e-01,\n",
       "         -3.8310e-01, -1.3635e-01,  8.6395e-01,  6.6278e-01, -8.6339e-01,\n",
       "          4.0562e-01, -3.3994e-01,  9.8333e-03, -1.5219e-01,  9.2872e-04,\n",
       "         -6.1607e-01, -1.3542e-01, -3.1598e-02,  2.3298e-01, -2.2722e-01,\n",
       "         -1.2424e+00,  4.2805e-01,  1.2294e-01, -3.8889e-01,  2.3461e-01,\n",
       "         -1.4831e-01, -4.0289e-01,  4.7073e-01, -7.3463e-02, -4.2337e-01,\n",
       "         -1.6074e-01, -1.3596e-01, -3.1426e-02, -2.5417e-01,  1.8968e-02,\n",
       "          1.8794e-01,  1.4758e-01, -2.8404e-01, -8.8069e-02,  4.5889e-01,\n",
       "         -3.5626e-02,  8.6572e-01,  2.5779e-03, -7.9225e-01, -1.6146e-02,\n",
       "         -2.4708e-01, -8.3802e-02,  3.7570e-01, -6.5372e-03, -2.2907e-01,\n",
       "          3.2578e-01, -1.9606e-01,  7.5748e-02,  5.5525e-01, -7.8517e-02,\n",
       "          6.9495e-01, -4.9127e-02, -4.2009e-02,  6.2171e-01,  1.0944e-01,\n",
       "         -2.8721e-01, -2.0108e-01,  3.4188e-01,  1.4723e-01,  5.2169e-01,\n",
       "          3.4934e-01, -6.1636e-01, -6.2249e-02,  4.7600e-01, -6.6296e-01,\n",
       "         -8.2984e-02,  1.0151e-01, -7.2916e-02, -1.9963e-01, -3.8767e-02,\n",
       "          1.4571e-01, -1.0573e-01, -3.6633e-01, -3.1252e-02, -3.7593e-01,\n",
       "         -3.3972e-02,  6.2288e-03, -9.0044e-02,  5.3838e-01, -5.2033e-01,\n",
       "         -2.9378e-01,  6.3146e-01, -2.8208e-01,  2.1697e-01, -1.2091e-01,\n",
       "          4.0242e-01,  5.0220e-01,  1.4731e-01, -1.9719e-01, -7.4286e-01,\n",
       "          1.1512e-01, -6.1915e-02,  1.2693e-01,  4.4106e-01, -5.2001e-01,\n",
       "          2.5600e-01, -2.8616e-01, -8.5689e-01,  1.7643e-01, -1.1765e-01,\n",
       "         -9.2024e-02,  2.6684e-01,  4.4530e-01,  2.7766e-01,  3.2873e-02,\n",
       "         -5.5395e-01, -8.2317e-01,  1.3844e-01, -8.1028e-02, -5.3285e-02,\n",
       "         -2.7322e-01,  1.7203e-01,  1.2118e-01,  2.2600e-01,  2.3044e-01,\n",
       "         -1.6194e-01,  5.1804e-01,  1.8878e-01,  3.5959e-01, -1.9229e-01,\n",
       "          2.4488e-01,  3.0569e-01,  2.6507e-01, -4.0924e-01, -3.8084e-01,\n",
       "          3.2372e-02,  5.6241e-01,  7.3325e-02, -4.1943e-02, -2.0832e-01,\n",
       "         -5.3963e-02, -6.1281e-02, -2.6085e-01,  1.8123e+00, -1.2204e-01,\n",
       "          3.9232e-01,  4.9060e-01,  3.6305e-02,  1.5733e-01, -4.8424e-01,\n",
       "         -5.8095e-01,  2.5153e-02,  3.4728e-01, -1.1065e-01,  3.3906e-02,\n",
       "         -2.8192e-01,  3.0827e-01,  1.4438e-01, -9.5595e-02, -1.0711e-02,\n",
       "          1.1559e-01, -5.8013e-01,  1.6017e-01, -6.0630e-01, -2.3979e-01,\n",
       "          2.1005e-01,  1.1964e-01,  1.6173e-01,  5.7587e-01,  1.8042e-02,\n",
       "         -5.5967e-01, -2.0416e-01,  6.5927e-02, -3.6402e-01,  3.6989e-02,\n",
       "          3.4152e-01,  6.0300e-01,  9.1675e-02,  2.5420e-01, -2.0660e-01,\n",
       "         -2.7551e-01, -4.5477e-01,  2.8603e-02, -1.5006e-01, -2.8382e-01,\n",
       "          9.8603e-01,  2.5817e-01, -1.8044e-03,  2.9267e-02,  2.6480e-01,\n",
       "         -4.2672e-01,  5.7459e-02,  6.7081e-01, -2.1847e-01, -1.2097e-01,\n",
       "          4.8630e-02, -2.3481e-01, -6.0064e-02, -1.2966e-01,  2.5211e-01,\n",
       "         -3.3443e-01, -5.5901e-01,  9.4865e-02,  3.5860e-01,  4.1029e-01,\n",
       "          6.6429e-02, -7.2827e-01, -6.4625e-03, -4.9302e-01,  1.7136e-01,\n",
       "         -9.2036e-02, -4.8961e-02,  1.0724e-01,  2.3339e-01,  2.6572e-02,\n",
       "          4.7007e-01, -8.3690e-02,  9.1468e-02, -5.2142e-02, -1.3378e-01,\n",
       "         -2.5042e-02, -2.9027e-03, -2.3868e+00, -9.9845e-02,  1.5638e-01,\n",
       "          4.5395e-01, -5.3079e-03, -2.4671e-01, -2.7484e-01,  3.9612e-01,\n",
       "         -4.9896e-02,  1.4630e-01, -1.8939e-01,  3.1593e-01,  1.6192e-01,\n",
       "          2.7646e-02,  1.9567e-01,  4.8286e-01,  3.7148e-01, -6.1057e-01,\n",
       "         -3.1603e-01, -4.5141e-01, -2.5178e-01, -3.3850e-01, -2.3986e-01,\n",
       "         -3.6721e-01, -2.7216e-01,  3.2412e-01,  1.6941e-01, -3.2915e-01,\n",
       "          2.7436e-01, -4.2832e-01,  1.1255e-01, -1.8318e-01,  3.8164e-01,\n",
       "          6.4424e-01, -5.0499e-01, -6.3643e-02, -5.1924e-02, -1.1560e-02,\n",
       "         -4.1425e-01,  9.6883e-02,  1.5106e-01,  6.0981e-01,  5.1986e-01,\n",
       "          6.0728e-01, -2.3092e-01,  7.1567e-02,  6.6007e-02, -3.8416e-01,\n",
       "          5.9576e-01, -8.3008e-02, -9.0335e-03,  1.9589e-01,  1.5522e-01,\n",
       "          1.1354e-01,  7.4279e-01,  3.4238e-01, -1.4272e-01,  1.5120e-01,\n",
       "          9.7664e-02, -1.3090e-01, -3.6027e-02, -3.5837e-02, -5.4736e-01,\n",
       "          4.9373e-02,  4.3410e-01, -4.9740e-01, -1.7775e-02,  2.6654e-01,\n",
       "          4.0269e-01, -3.4783e-01, -5.3617e-02, -4.0317e-02,  1.4755e-01,\n",
       "          4.1013e-02, -2.4127e-01,  1.8908e-01,  4.9430e-03,  1.8272e-01,\n",
       "          2.4605e-01,  1.5481e-01,  4.3881e-02, -6.8330e-02,  1.5480e-01,\n",
       "         -7.6233e-03,  5.1967e-01, -6.4584e+00, -3.9756e-01, -1.3627e-01,\n",
       "         -3.5031e-01, -2.9978e-01, -4.3234e-01,  3.3652e-01,  1.1543e-01,\n",
       "         -1.5047e-01,  6.1329e-01,  7.5059e-03, -3.5496e-01,  6.3550e-03,\n",
       "          4.3937e-02,  2.0455e-01,  7.6083e-01]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = hidden_states[-1][:, 0, :]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  8667,   117,  1139,  3676,  1110, 10509,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# two methods learn the same last hidden state\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\", padding=\"max_length\")   # 512\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-4.7624e-01,  1.1550e-01,  1.1242e-01, -5.7556e-01, -2.4503e-01,\n",
       "          8.4391e-02,  5.3658e-01,  4.9711e-01, -4.0918e-01, -1.6279e-01,\n",
       "         -2.8653e-02,  5.8704e-02,  2.1708e-01, -1.1108e-02,  7.4637e-02,\n",
       "          3.4948e-01,  2.0475e-01,  1.8479e-02,  2.1207e-01, -4.7727e-02,\n",
       "         -1.3610e-01, -2.6399e-01, -2.1643e-02, -3.0580e-01,  3.2172e-01,\n",
       "         -2.0820e-01, -2.3615e-01, -1.6109e-01,  1.0481e-01, -2.9942e-01,\n",
       "         -1.0333e-01,  5.3430e-01, -1.8657e-01, -4.1273e-01,  2.6009e-02,\n",
       "          8.9587e-02,  1.0829e-01, -2.5235e-02,  2.0665e-01,  5.0784e-01,\n",
       "         -3.6589e-01,  1.5930e-01,  1.8801e-01,  2.6812e-02, -1.4873e-01,\n",
       "         -1.5996e-01, -2.4484e+00, -2.2372e-01, -4.6786e-01, -3.4599e-01,\n",
       "         -6.4978e-02,  1.4295e-01,  1.6354e-01,  1.7121e-01, -1.9327e-01,\n",
       "          3.7022e-01, -5.4412e-01,  2.0715e-01,  5.2038e-01,  1.3504e-01,\n",
       "         -2.7290e-02, -5.9998e-02, -9.9373e-02,  1.2187e-01, -2.5336e-02,\n",
       "          4.9320e-01, -5.6170e-01,  8.1200e-01, -3.2150e-01,  6.0890e-01,\n",
       "         -7.2384e-01,  1.2692e-02,  1.5561e-01, -1.7363e-01, -2.9189e-01,\n",
       "         -2.6666e-01, -7.2970e-02, -8.3492e-02, -2.1541e-01, -2.3849e-01,\n",
       "          1.6891e-02, -3.5241e-01,  7.4463e-02, -3.2028e-01,  4.9354e-01,\n",
       "          5.3577e-01, -1.0973e-01, -3.1692e-01,  8.2287e-03,  7.7993e-01,\n",
       "         -3.8841e-02, -2.4210e-01, -1.3188e-01, -6.1443e-02,  1.8719e-01,\n",
       "          1.5157e-01,  2.2129e-01,  6.1553e-02,  3.9795e-01,  8.2185e-02,\n",
       "         -5.4557e-01,  2.2111e-02,  1.4660e-01, -1.0377e-01,  1.6798e-01,\n",
       "         -1.2479e-01, -3.4161e-01, -1.2931e-01, -3.7190e-02, -2.1460e+00,\n",
       "          3.6114e-01,  2.3807e-01, -1.3247e-01, -3.3372e-01,  2.1278e-01,\n",
       "          2.8451e-01,  1.9800e-01,  6.9342e-02, -1.8462e-01, -4.3570e-01,\n",
       "          1.5788e-01,  1.5433e-01,  1.8099e-01, -1.5875e-01,  1.3119e-01,\n",
       "          5.1972e-01, -2.3351e-03, -2.4131e-01,  4.2394e-01,  2.1200e-01,\n",
       "          1.8222e-01,  6.1231e-01,  1.7458e-01, -2.0838e-02, -6.5461e-04,\n",
       "         -5.0022e-01,  4.8477e-01,  3.4151e-01, -9.1899e-02, -3.0757e-01,\n",
       "         -4.8270e-01, -3.9578e-01, -2.8264e+00,  1.2624e-01,  8.7297e-01,\n",
       "          4.5581e-01, -8.4963e-01,  5.6020e-01, -1.3979e-01,  7.1904e-01,\n",
       "          6.9633e-02, -3.0753e-01, -1.5228e-01,  3.1401e-01, -7.5921e-01,\n",
       "          4.2926e-01,  2.6339e-01, -3.2865e-01,  8.1524e-03,  4.0691e-01,\n",
       "          2.9297e-01,  3.3483e-01,  3.2106e-01,  5.2289e-02,  1.3241e-01,\n",
       "         -3.4927e-01, -1.5889e-01,  6.7754e-02, -1.7619e-01, -1.9277e-01,\n",
       "         -4.8428e-03, -1.8275e-01,  4.7482e-01,  1.2520e-01,  3.5838e-01,\n",
       "         -6.2690e-01, -1.6290e-01,  1.4096e-01,  1.5032e-01,  2.2932e-01,\n",
       "         -3.5920e-01,  4.8154e-01,  2.3448e-01, -2.4403e-01,  2.5122e-01,\n",
       "          1.0059e-01,  2.9600e-01, -5.1344e-01,  2.5889e-01, -2.2210e-01,\n",
       "         -1.4207e-01,  7.9166e-02, -1.1993e-02, -2.7736e-01,  4.1982e-02,\n",
       "          1.8882e-01,  2.9208e-01, -6.1503e-01,  3.1595e-01,  5.6018e-01,\n",
       "         -9.5873e-02,  2.0107e-01, -1.6868e-01,  1.7126e-01,  7.5815e-02,\n",
       "          3.5901e+00,  2.1255e-01,  1.5029e-02,  6.1841e-01, -1.7740e-02,\n",
       "          4.5367e-02,  1.6076e-01,  1.9663e-01, -2.5325e-01,  4.7615e-01,\n",
       "         -2.2850e-01,  1.0860e+00, -4.4635e-01, -2.9348e-01, -1.5295e-01,\n",
       "          3.3980e-02, -5.7536e-02, -3.2032e-02,  4.5337e-01, -3.0298e-01,\n",
       "          3.3804e-01,  2.2662e-01,  3.1851e-01, -1.9674e-01, -6.3473e-01,\n",
       "          1.6998e-01,  1.6466e-01, -1.7648e-01,  8.0789e-02,  4.5683e-02,\n",
       "          6.5488e-02,  5.8420e-02,  4.2670e-02, -3.0742e-02,  2.0669e-01,\n",
       "         -1.4432e-01,  4.3681e-01,  5.9786e-02, -2.6029e-01, -2.3127e-01,\n",
       "         -5.0518e-01,  1.0435e+00, -1.2091e-01, -2.4971e-01,  8.4993e-02,\n",
       "          2.9295e-01, -1.5486e-01,  1.2291e-01, -7.4184e-01, -2.9787e-01,\n",
       "         -1.1980e-01,  4.7295e-02,  1.3128e-01,  1.0508e-01, -4.7625e-01,\n",
       "         -1.1862e-01,  2.1419e-01,  5.6038e-01, -3.5879e-01, -4.4270e-01,\n",
       "          1.8804e-01, -1.1989e-01,  1.7226e-02,  5.2940e-01, -2.8251e-01,\n",
       "          2.6407e-02,  1.6196e-01, -2.8187e-01, -3.2083e+00, -4.2521e-01,\n",
       "          1.7435e-01,  4.1709e-01,  2.2784e-01, -5.4080e-01, -3.0949e-01,\n",
       "          3.9267e-01, -2.1743e-02, -2.9189e-01, -1.1681e-01, -1.9837e-01,\n",
       "         -7.2170e-02,  1.4689e-01, -4.1932e-01,  1.3957e-01, -3.4362e-01,\n",
       "          2.3228e-01,  2.1457e-01, -1.3057e-01,  5.9171e-02,  5.5401e-01,\n",
       "         -6.3272e-01, -2.6936e-01,  6.4403e-01, -3.5659e-02, -3.2368e-01,\n",
       "          1.7447e-02,  2.3592e-01, -5.9322e-02, -2.4444e-01, -9.0071e-02,\n",
       "         -7.3795e-02, -1.7223e-01, -3.0483e-01, -4.0158e+00,  5.6193e-02,\n",
       "         -3.0848e-02, -1.3590e-01,  7.7842e-02, -1.0643e-01,  4.7385e-01,\n",
       "         -6.0634e-02,  4.9041e-01,  6.1780e-02, -1.7514e-01, -3.7478e-01,\n",
       "         -4.2694e-01, -8.2553e-02, -1.1687e-01, -3.4605e-02,  1.1833e-01,\n",
       "          2.3628e-01,  5.2108e-02,  2.3280e-01,  2.1575e-01,  1.9993e-01,\n",
       "          2.9014e-01,  9.0656e-03,  1.4727e-01,  4.3309e-01, -4.4000e-02,\n",
       "         -9.5601e-02, -3.4744e-01, -1.4977e-01, -3.3792e-02, -2.1585e-01,\n",
       "          5.7591e-02,  8.3210e-02, -1.1296e-01, -4.1011e-01,  2.0870e-01,\n",
       "          1.5449e-01,  5.5197e-01,  1.5462e-01, -3.8144e-01,  6.5238e-01,\n",
       "          3.2656e-01,  5.0548e-02,  4.7506e-01,  1.5517e-01, -2.5233e-01,\n",
       "         -4.0647e-01,  1.1413e-01,  4.5286e-02, -5.8965e-01,  5.9818e-02,\n",
       "          9.4316e-01, -2.9295e-01,  9.3985e-01, -3.3613e-02, -1.7120e-02,\n",
       "          2.4561e-01,  5.4440e-01,  1.5586e-01,  7.4958e-01, -5.9065e-02,\n",
       "         -4.1819e-01, -2.2344e-01,  3.5482e-01, -2.9947e-01,  2.0406e-01,\n",
       "         -3.7246e-01, -3.9314e-02,  1.2546e-01, -2.4329e-01,  7.7731e-02,\n",
       "          1.1129e-01, -5.8960e-01,  2.5220e-02, -1.2206e-02, -3.8573e-01,\n",
       "          7.7063e-02, -6.3751e-02,  1.8099e-01, -4.7636e-01, -1.2697e-01,\n",
       "          3.1721e-01,  6.7667e-01, -2.6372e-01,  5.3201e-01, -1.3893e-01,\n",
       "         -4.1748e-02, -2.4655e-01,  3.6538e-01, -2.0221e-01,  4.2030e-01,\n",
       "          8.8757e-02,  7.8413e-02, -3.2370e-01,  5.6662e-01,  6.3215e-01,\n",
       "         -8.0259e-01,  6.9987e-01, -3.8572e-01,  4.6251e-02, -5.2781e-01,\n",
       "         -6.1187e-01,  7.6018e-02, -1.7971e-01, -1.4585e-01, -6.1393e-01,\n",
       "          5.8376e-02,  6.1459e-01,  3.2983e-01,  2.3522e-01,  1.9385e-01,\n",
       "         -4.3783e-01,  1.4180e-01,  6.9767e-01, -2.1719e-01, -4.0905e-01,\n",
       "          3.2053e-01,  1.3304e-01,  1.4721e-01,  1.3865e-01,  3.0314e-01,\n",
       "         -1.5515e-01,  1.2555e-01,  1.7551e-01,  5.4261e-01,  3.3466e-01,\n",
       "         -1.0954e-02, -6.1516e-01, -8.7878e-02, -1.3672e-01, -4.5312e-01,\n",
       "         -2.5234e-01, -4.0823e-01,  2.0254e-01, -5.9867e-02, -2.0970e-01,\n",
       "          7.7189e-01,  4.0122e-01,  1.7754e-01,  7.4776e-02, -2.3066e-01,\n",
       "         -1.1238e-01,  2.2088e-01, -1.5820e-01,  6.3689e-01,  1.1074e-01,\n",
       "         -3.8310e-01, -1.3635e-01,  8.6395e-01,  6.6278e-01, -8.6339e-01,\n",
       "          4.0562e-01, -3.3994e-01,  9.8337e-03, -1.5219e-01,  9.2904e-04,\n",
       "         -6.1607e-01, -1.3542e-01, -3.1598e-02,  2.3298e-01, -2.2722e-01,\n",
       "         -1.2424e+00,  4.2805e-01,  1.2294e-01, -3.8889e-01,  2.3460e-01,\n",
       "         -1.4831e-01, -4.0289e-01,  4.7073e-01, -7.3463e-02, -4.2337e-01,\n",
       "         -1.6073e-01, -1.3596e-01, -3.1426e-02, -2.5417e-01,  1.8967e-02,\n",
       "          1.8794e-01,  1.4758e-01, -2.8404e-01, -8.8068e-02,  4.5890e-01,\n",
       "         -3.5627e-02,  8.6572e-01,  2.5776e-03, -7.9225e-01, -1.6146e-02,\n",
       "         -2.4708e-01, -8.3802e-02,  3.7570e-01, -6.5378e-03, -2.2907e-01,\n",
       "          3.2578e-01, -1.9606e-01,  7.5748e-02,  5.5525e-01, -7.8516e-02,\n",
       "          6.9495e-01, -4.9127e-02, -4.2009e-02,  6.2171e-01,  1.0944e-01,\n",
       "         -2.8721e-01, -2.0108e-01,  3.4188e-01,  1.4723e-01,  5.2169e-01,\n",
       "          3.4934e-01, -6.1636e-01, -6.2249e-02,  4.7600e-01, -6.6296e-01,\n",
       "         -8.2984e-02,  1.0151e-01, -7.2916e-02, -1.9963e-01, -3.8767e-02,\n",
       "          1.4571e-01, -1.0573e-01, -3.6633e-01, -3.1252e-02, -3.7593e-01,\n",
       "         -3.3972e-02,  6.2289e-03, -9.0045e-02,  5.3838e-01, -5.2033e-01,\n",
       "         -2.9378e-01,  6.3146e-01, -2.8208e-01,  2.1697e-01, -1.2091e-01,\n",
       "          4.0242e-01,  5.0220e-01,  1.4731e-01, -1.9719e-01, -7.4286e-01,\n",
       "          1.1512e-01, -6.1915e-02,  1.2693e-01,  4.4106e-01, -5.2001e-01,\n",
       "          2.5600e-01, -2.8616e-01, -8.5689e-01,  1.7643e-01, -1.1765e-01,\n",
       "         -9.2024e-02,  2.6684e-01,  4.4530e-01,  2.7766e-01,  3.2873e-02,\n",
       "         -5.5395e-01, -8.2317e-01,  1.3844e-01, -8.1028e-02, -5.3285e-02,\n",
       "         -2.7322e-01,  1.7203e-01,  1.2118e-01,  2.2600e-01,  2.3044e-01,\n",
       "         -1.6194e-01,  5.1804e-01,  1.8878e-01,  3.5959e-01, -1.9229e-01,\n",
       "          2.4488e-01,  3.0569e-01,  2.6507e-01, -4.0924e-01, -3.8084e-01,\n",
       "          3.2371e-02,  5.6241e-01,  7.3325e-02, -4.1943e-02, -2.0832e-01,\n",
       "         -5.3962e-02, -6.1281e-02, -2.6085e-01,  1.8123e+00, -1.2204e-01,\n",
       "          3.9232e-01,  4.9060e-01,  3.6305e-02,  1.5733e-01, -4.8424e-01,\n",
       "         -5.8095e-01,  2.5153e-02,  3.4728e-01, -1.1065e-01,  3.3906e-02,\n",
       "         -2.8192e-01,  3.0827e-01,  1.4438e-01, -9.5594e-02, -1.0711e-02,\n",
       "          1.1559e-01, -5.8013e-01,  1.6017e-01, -6.0630e-01, -2.3979e-01,\n",
       "          2.1005e-01,  1.1964e-01,  1.6173e-01,  5.7587e-01,  1.8042e-02,\n",
       "         -5.5967e-01, -2.0416e-01,  6.5927e-02, -3.6402e-01,  3.6989e-02,\n",
       "          3.4152e-01,  6.0300e-01,  9.1675e-02,  2.5420e-01, -2.0660e-01,\n",
       "         -2.7551e-01, -4.5477e-01,  2.8603e-02, -1.5006e-01, -2.8382e-01,\n",
       "          9.8603e-01,  2.5817e-01, -1.8041e-03,  2.9267e-02,  2.6480e-01,\n",
       "         -4.2671e-01,  5.7459e-02,  6.7081e-01, -2.1847e-01, -1.2097e-01,\n",
       "          4.8630e-02, -2.3481e-01, -6.0065e-02, -1.2966e-01,  2.5211e-01,\n",
       "         -3.3443e-01, -5.5901e-01,  9.4865e-02,  3.5860e-01,  4.1029e-01,\n",
       "          6.6429e-02, -7.2827e-01, -6.4624e-03, -4.9302e-01,  1.7136e-01,\n",
       "         -9.2035e-02, -4.8961e-02,  1.0724e-01,  2.3339e-01,  2.6572e-02,\n",
       "          4.7007e-01, -8.3690e-02,  9.1468e-02, -5.2142e-02, -1.3378e-01,\n",
       "         -2.5042e-02, -2.9028e-03, -2.3868e+00, -9.9845e-02,  1.5638e-01,\n",
       "          4.5395e-01, -5.3079e-03, -2.4671e-01, -2.7484e-01,  3.9612e-01,\n",
       "         -4.9896e-02,  1.4630e-01, -1.8939e-01,  3.1593e-01,  1.6192e-01,\n",
       "          2.7645e-02,  1.9567e-01,  4.8286e-01,  3.7148e-01, -6.1057e-01,\n",
       "         -3.1603e-01, -4.5141e-01, -2.5178e-01, -3.3850e-01, -2.3986e-01,\n",
       "         -3.6721e-01, -2.7216e-01,  3.2412e-01,  1.6941e-01, -3.2915e-01,\n",
       "          2.7436e-01, -4.2832e-01,  1.1255e-01, -1.8318e-01,  3.8164e-01,\n",
       "          6.4424e-01, -5.0499e-01, -6.3643e-02, -5.1924e-02, -1.1560e-02,\n",
       "         -4.1425e-01,  9.6883e-02,  1.5106e-01,  6.0981e-01,  5.1986e-01,\n",
       "          6.0728e-01, -2.3092e-01,  7.1567e-02,  6.6007e-02, -3.8416e-01,\n",
       "          5.9576e-01, -8.3008e-02, -9.0337e-03,  1.9589e-01,  1.5522e-01,\n",
       "          1.1354e-01,  7.4279e-01,  3.4238e-01, -1.4272e-01,  1.5120e-01,\n",
       "          9.7664e-02, -1.3090e-01, -3.6027e-02, -3.5836e-02, -5.4736e-01,\n",
       "          4.9373e-02,  4.3410e-01, -4.9740e-01, -1.7775e-02,  2.6654e-01,\n",
       "          4.0269e-01, -3.4783e-01, -5.3617e-02, -4.0317e-02,  1.4755e-01,\n",
       "          4.1013e-02, -2.4127e-01,  1.8908e-01,  4.9428e-03,  1.8272e-01,\n",
       "          2.4605e-01,  1.5481e-01,  4.3881e-02, -6.8330e-02,  1.5480e-01,\n",
       "         -7.6233e-03,  5.1967e-01, -6.4584e+00, -3.9756e-01, -1.3627e-01,\n",
       "         -3.5031e-01, -2.9978e-01, -4.3234e-01,  3.3652e-01,  1.1543e-01,\n",
       "         -1.5047e-01,  6.1329e-01,  7.5056e-03, -3.5496e-01,  6.3552e-03,\n",
       "          4.3937e-02,  2.0455e-01,  7.6083e-01]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "# outputs[1]\n",
    "b = outputs[1][-1][:, 0, :]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last hidden state\n",
    "outputs[1][-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_config = ResNetConfig(block = Bottleneck,\n",
    "                               n_blocks = [3, 4, 6, 3],\n",
    "                               channels = [64, 128, 256, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = models.resnet50(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(resnet50_config, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(pretrained_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 25,557,032 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 512)\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Hello world\"]\n",
    "\n",
    "embeddings = clip_model.encode(sentences)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct VAE class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, zsize, output_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.zsize = zsize\n",
    "        self.fc1 = nn.Linear(zsize, zsize)   # 4 * 4 is the current size of the image\n",
    "        self.fc2 = nn.Linear(zsize, zsize)\n",
    "\n",
    "        ######\n",
    "        # multi-tasks sub-networks\n",
    "        self.fc_misogynous = nn.Linear(zsize, output_dim)\n",
    "        self.fc_shaming = nn.Linear(zsize, output_dim)\n",
    "        self.fc_stereotype = nn.Linear(zsize, output_dim)\n",
    "        self.fc_objectification = nn.Linear(zsize, output_dim)\n",
    "        self.fc_violence = nn.Linear(zsize, output_dim)\n",
    "        \n",
    "        # language pre-trained model\n",
    "#         self.bert_pretrained = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "#         self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        \n",
    "#         set_parameter_requires_grad_false(self.bert_pretrained)\n",
    "        \n",
    "        # image pre-trained model\n",
    "#         ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])\n",
    "#         resnet50_config = ResNetConfig(block = Bottleneck,\n",
    "#                                n_blocks = [3, 4, 6, 3],\n",
    "#                                channels = [64, 128, 256, 512])\n",
    "#         pretrained_model = models.resnet50(pretrained = True)\n",
    "        \n",
    "#         self.resnet_pretrained = ResNet(resnet50_config, 1000)\n",
    "#         self.resnet_pretrained.load_state_dict(pretrained_model.state_dict())\n",
    "        \n",
    "#         set_parameter_requires_grad_false(self.resnet_pretrained)\n",
    "        \n",
    "        # encoder layers\n",
    "        self.enc_txt_fc = nn.Linear(1024, int(0.5 * zsize))\n",
    "        self.enc_img_fc1 = nn.Linear(512, int(0.5 * zsize))\n",
    "#         self.enc_img_fc2 = nn.Linear(1024, int(0.5 * zsize))\n",
    "        \n",
    "        # decoder layers\n",
    "        self.dec_txt_fc = nn.Linear(zsize, 1024)\n",
    "        self.dec_img_fc1 = nn.Linear(zsize, 512)\n",
    "#         self.dec_img_fc2 = nn.Linear(1024, 2048)\n",
    "\n",
    "        # batch normalizations\n",
    "        self.enc_txt_bn = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "        self.enc_img_bn1 = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "#         self.enc_img_bn2 = nn.BatchNorm1d(num_features=int(0.5 * zsize))\n",
    "        \n",
    "        self.dec_txt_bn = nn.BatchNorm1d(num_features=1024)\n",
    "        self.dec_img_bn1 = nn.BatchNorm1d(num_features=512)\n",
    "#         self.dec_img_bn2 = nn.BatchNorm1d(num_features=2048)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout_txt_enc = nn.Dropout(0.2)\n",
    "        self.dropout_img_enc = nn.Dropout(0.2)\n",
    "        self.dropout_txt_dec = nn.Dropout(0.2)\n",
    "        self.dropout_img_dec = nn.Dropout(0.2)\n",
    "        \n",
    "        \n",
    "    def img_encode(self, x_img):\n",
    "#         _, x_img = self.resnet_pretrained(x_img)\n",
    "        x_img = F.relu(self.dropout_img_enc(self.enc_img_bn1(self.enc_img_fc1(x_img))))\n",
    "#         x_img = F.relu(self.enc_img_fc2(x_img))\n",
    "        \n",
    "        return x_img   # [bs, 2048]\n",
    "\n",
    "    def txt_encode(self, x_txt):\n",
    "\n",
    "#         inputs = self.tokenizer(x_txt, return_tensors=\"pt\", padding=\"max_length\")   # 512\n",
    "#         x_txt = self.bert_pretrained(**inputs, output_hidden_states=True)\n",
    "#         # last hidden state\n",
    "#         x_txt = x_txt[1][-1]   # [bs, max_length, 768]\n",
    "#         x_txt = x_txt[:, 0, :]   # [CLS] token embedding represent the whole sentence\n",
    "        x_txt = x_txt.view(x_txt.shape[0], 1024)\n",
    "        x_txt = F.relu(self.dropout_txt_enc(self.enc_txt_bn(self.enc_txt_fc(x_txt))))\n",
    "        return x_txt   # [bs, 0.5 * zsize]\n",
    "\n",
    "    def encode(self, x_img, x_txt):\n",
    "        \n",
    "        x_img = self.img_encode(x_img)\n",
    "        \n",
    "        x_txt = self.txt_encode(x_txt)\n",
    "        \n",
    "        # concate x_img and x_txt\n",
    "        x = torch.cat((x_txt, x_img), 1)\n",
    "        \n",
    "        h1 = self.fc1(x)   # mu\n",
    "        h2 = self.fc2(x)   # logvar\n",
    "        return h1, h2\n",
    "    \n",
    "    def subtask_misogynous(self, z):\n",
    "        \n",
    "        h = self.fc_misogynous(z)\n",
    "        return h\n",
    "    \n",
    "    def subtask_shaming(self, z):\n",
    "        \n",
    "        h = self.fc_shaming(z)\n",
    "        return h\n",
    "    \n",
    "    def subtask_stereotype(self, z):\n",
    "\n",
    "        h = self.fc_stereotype(z)\n",
    "        return h\n",
    "    \n",
    "    def subtask_objectification(self, z):\n",
    "\n",
    "        h = self.fc_objectification(z)\n",
    "        return h\n",
    "    \n",
    "    def subtask_violence(self, z):\n",
    "\n",
    "        h = self.fc_violence(z)\n",
    "        return h\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, x):\n",
    "#         x = x.view(x.shape[0], self.zsize)   # flatten\n",
    "\n",
    "        # Decoding txt\n",
    "        dec_x_txt = F.relu(self.dropout_txt_dec(self.dec_txt_bn(self.dec_txt_fc(x))))\n",
    "        \n",
    "        # Decoding img\n",
    "        dec_x_img = F.relu(self.dropout_img_dec(self.dec_img_bn1(self.dec_img_fc1(x))))\n",
    "#         dec_x_img = F.relu(self.dec_img_fc2(dec_x_img))\n",
    "        \n",
    "        return dec_x_img, dec_x_txt\n",
    "\n",
    "    def forward(self, x_img, x_txt):\n",
    "        mu, logvar = self.encode(x_img, x_txt)\n",
    "        mu = mu.squeeze()\n",
    "        logvar = logvar.squeeze()\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "\n",
    "        y_misogynous = self.subtask_misogynous(z)\n",
    "        y_shaming = self.subtask_shaming(z)\n",
    "        y_stereotype = self.subtask_stereotype(z)\n",
    "        y_objectification = self.subtask_objectification(z)\n",
    "        y_violence = self.subtask_violence(z)\n",
    "        \n",
    "        y_pred = dict()\n",
    "        y_pred[\"misogynous\"] = y_misogynous\n",
    "        y_pred[\"shaming\"] = y_shaming\n",
    "        y_pred[\"stereotype\"] = y_stereotype\n",
    "        y_pred[\"objectification\"] = y_objectification\n",
    "        y_pred[\"violence\"] = y_violence\n",
    "        \n",
    "        dec_x_img, dec_x_txt = self.decode(z.view(-1, self.zsize))\n",
    "        \n",
    "        return dec_x_img, dec_x_txt, mu, logvar, y_pred\n",
    "\n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (fc1): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (fc_misogynous): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (fc_shaming): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (fc_stereotype): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (fc_objectification): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (fc_violence): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (enc_txt_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (enc_img_fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (dec_txt_fc): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (dec_img_fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (enc_txt_bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (enc_img_bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dec_txt_bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dec_img_bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout_txt_enc): Dropout(p=0.2, inplace=False)\n",
      "  (dropout_img_enc): Dropout(p=0.2, inplace=False)\n",
      "  (dropout_txt_dec): Dropout(p=0.2, inplace=False)\n",
      "  (dropout_img_dec): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vae = VAE(1024)   # 5 layers\n",
    "# vae = VAE(1024, 5)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_size = 128\n",
    "# im_size = 256\n",
    "im_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x_img, recon_x_txt, x_img, x_txt, mu, logvar):\n",
    "    BCE_img = torch.mean((recon_x_img - x_img)**2)\n",
    "    BCE_txt = torch.mean((recon_x_txt - x_txt)**2)\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.mean(torch.mean(1 + logvar - mu.pow(2) - logvar.exp(), 1))\n",
    "    return BCE_img, BCE_txt, KLD * 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"VAE_result_clip_laser_dropout_saved_model.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    name_dict = dict()\n",
    "    name_dict[\"misogynous\"] = 0\n",
    "    name_dict[\"shaming\"] = 1\n",
    "    name_dict[\"stereotype\"] = 2\n",
    "    name_dict[\"objectification\"] = 3\n",
    "    name_dict[\"violence\"] = 4\n",
    "    \n",
    "    #batch_size = 32\n",
    "    z_size = 512\n",
    "#     z_size = 1024\n",
    "    vae = VAE(z_size)\n",
    "    vae.cuda()\n",
    "    vae.train()\n",
    "    vae.weight_init(mean=0, std=0.02)\n",
    "#     input_dim = 128 * 128\n",
    "#     input_dim = 256 * 256\n",
    "#     input_dim = 224 * 224\n",
    "\n",
    "#     lr = 0.0005\n",
    "    lr = 0.0001\n",
    "\n",
    "    vae_optimizer = optim.Adam(vae.parameters(), lr=lr, betas=(0.5, 0.999), weight_decay=1e-5)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion.to(device)\n",
    " \n",
    "    train_epoch = 30\n",
    "\n",
    "    \n",
    "    dataloader = train_iterator\n",
    "    \n",
    "    f1_max = 0\n",
    "    max_acc = 0\n",
    "    \n",
    "    for epoch in range(train_epoch):\n",
    "        vae.train()\n",
    "\n",
    "#         with open('data_fold_%d.pkl' % (epoch % 5), 'rb') as pkl:\n",
    "#             data_train = pickle.load(pkl)\n",
    "\n",
    "#         print(\"Train set size:\", len(data_train))\n",
    "\n",
    "#         random.shuffle(data_train)\n",
    "\n",
    "#         batches = batch_provider(data_train, batch_size, process_batch, report_progress=True)\n",
    "\n",
    "        rec_txt_loss = 0\n",
    "        rec_img_loss = 0\n",
    "        kl_loss = 0\n",
    "        subtask_misogynous_loss = 0\n",
    "        subtask_shaming_loss = 0\n",
    "        subtask_stereotype_loss = 0\n",
    "        subtask_objectification_loss = 0\n",
    "        subtask_violence_loss = 0\n",
    "\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        if (epoch + 1) % 8 == 0:\n",
    "            vae_optimizer.param_groups[0]['lr'] /= 4\n",
    "#             print(\"learning rate change!\")\n",
    "            f.write(\"learning rate change! The learning rate is %1.4f now\\n\" % (lr))\n",
    "\n",
    "#         i = 0\n",
    "        acc = 0\n",
    "        num = 0\n",
    "        for i, data in tqdm(enumerate(dataloader, 0), desc='iterations'):\n",
    "        #for x in batches:\n",
    "            vae.train()\n",
    "            \n",
    "            #inputs, classes = data\n",
    "            img_inputs = data['meme']\n",
    "            img_inputs = img_inputs.to(device)\n",
    "#             _, img_inputs = resnet_pretrained(img_inputs)\n",
    "#             img_inputs = clip_pretrained.encode(img_inputs)\n",
    "#             print(img_inputs.shape)\n",
    "            \n",
    "#             txt_ids = data['text_ids']\n",
    "#             tokens_tensor = txt_ids['input_ids']\n",
    "#             segments_tensor = txt_ids['attention_mask']\n",
    "            \n",
    "#             tokens_tensor = tokens_tensor.to(device).squeeze(axis=1)\n",
    "#             segments_tensor = segments_tensor.to(device).squeeze(axis=1)\n",
    "            txt_inputs = data[\"text\"]\n",
    "            txt_inputs = txt_inputs.to(device)\n",
    "            \n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 txt_inputs = bert_pretrained(tokens_tensor, segments_tensor, output_hidden_states=True)\n",
    "                \n",
    "#                 # last hidden state\n",
    "#                 txt_inputs = txt_inputs[1][-1]   # [bs, max_length, 768]\n",
    "#                 txt_inputs = txt_inputs[:, 0, :]   # [CLS] token embedding represent the whole sentence\n",
    "            \n",
    "            classes = data['labels']\n",
    "            \n",
    "            # multi-task labels\n",
    "            classes_misogynous = classes[:, 0]\n",
    "            classes_shaming = classes[:, 1]\n",
    "            classes_stereotype = classes[:, 2]\n",
    "            classes_objectification = classes[:, 3]\n",
    "            classes_violence = classes[:, 4]\n",
    "            #print(classes)\n",
    "#             inputs, classes = Variable(inputs.resize_(batch_size, input_dim)), Variable(classes)\n",
    "            \n",
    "            img_inputs, txt_inputs, classes_misogynous = Variable(img_inputs), Variable(txt_inputs), Variable(classes_misogynous)\n",
    "            classes_shaming = Variable(classes_shaming)\n",
    "            classes_stereotype = Variable(classes_stereotype)\n",
    "            classes_objectification = Variable(classes_stereotype)\n",
    "            classes_violence = Variable(classes_violence)\n",
    "        \n",
    "            img_inputs = img_inputs.to(device)\n",
    "            txt_inputs = txt_inputs.to(device)\n",
    "            classes_misogynous = classes_misogynous.to(device)\n",
    "            classes_shaming = classes_shaming.to(device)\n",
    "            classes_stereotype = classes_stereotype.to(device)\n",
    "            classes_objectification = classes_objectification.to(device)\n",
    "            classes_violence = classes_violence.to(device)\n",
    "            \n",
    "            vae.zero_grad()\n",
    "#             rec, mu, logvar = vae(x)\n",
    "            rec_img, rec_txt, mu, logvar, y_pred = vae(img_inputs, txt_inputs)\n",
    "\n",
    "            loss_re_img, loss_re_txt, loss_kl = loss_function(rec_img, rec_txt, img_inputs, txt_inputs, mu, logvar)\n",
    "            loss_subtask_misogynous = criterion(y_pred[\"misogynous\"], classes_misogynous)\n",
    "            loss_subtask_shaming = criterion(y_pred[\"shaming\"], classes_shaming)\n",
    "            loss_subtask_stereotype = criterion(y_pred[\"stereotype\"], classes_stereotype)\n",
    "            loss_subtask_objectification = criterion(y_pred[\"objectification\"], classes_objectification)\n",
    "            loss_subtask_violence = criterion(y_pred[\"violence\"], classes_violence)\n",
    "            \n",
    "            (loss_re_img + loss_re_txt + loss_kl + loss_subtask_misogynous \\\n",
    "             + loss_subtask_shaming + loss_subtask_stereotype + loss_subtask_objectification\\\n",
    "             + loss_subtask_violence).backward()\n",
    "            \n",
    "            vae_optimizer.step()\n",
    "            rec_img_loss += loss_re_img.item()\n",
    "            rec_txt_loss += loss_re_txt.item()\n",
    "            \n",
    "            kl_loss += loss_kl.item()\n",
    "            subtask_misogynous_loss += loss_subtask_misogynous.item()\n",
    "            subtask_shaming_loss += loss_subtask_shaming.item()\n",
    "            subtask_stereotype_loss += loss_subtask_stereotype.item()\n",
    "            subtask_objectification_loss += loss_subtask_objectification.item()\n",
    "            subtask_violence_loss += loss_subtask_violence.item()\n",
    "            \n",
    "            # Calculate batch accuracy\n",
    "            _, top_pred = y_pred[\"misogynous\"].topk(1, 1)\n",
    "            y = classes_misogynous.cpu()\n",
    "            batch_size = y.shape[0]\n",
    "            top_pred = top_pred.cpu().view(batch_size)\n",
    "            acc += sum(top_pred == y).item()\n",
    "            num += batch_size\n",
    "\n",
    "            #############################################\n",
    "\n",
    "#             os.makedirs('results_rec_64', exist_ok=True)\n",
    "#             os.makedirs('results_gen_64', exist_ok=True)\n",
    "\n",
    "            epoch_end_time = time.time()\n",
    "            per_epoch_ptime = epoch_end_time - epoch_start_time\n",
    "\n",
    "            # report losses and save samples each 60 iterations\n",
    "            m = len(dataloader)\n",
    "            i += 1\n",
    "            if i % m == 0:\n",
    "                rec_txt_loss /= m\n",
    "                rec_img_loss /= m\n",
    "                kl_loss /= m\n",
    "                subtask_misogynous_loss /= m\n",
    "                subtask_shaming_loss /= m\n",
    "                subtask_stereotype_loss /= m\n",
    "                subtask_objectification_loss /= m\n",
    "                subtask_violence_loss /= m\n",
    "                \n",
    "#                 print('\\n[%d/%d] - ptime: %.2f, rec img loss: %.9f, rec txt loss: %.9f, KL loss: %.9f, misogynous loss: %.9f, shaming loss: %.9f, stereotype loss: %.9f, objectification loss: %.9f, violence loss: %.9f' % (\n",
    "#                     (epoch + 1), train_epoch, per_epoch_ptime, rec_img_loss, rec_txt_loss, kl_loss, subtask_misogynous_loss, subtask_shaming_loss, subtask_stereotype_loss, subtask_objectification_loss, subtask_violence_loss))\n",
    "\n",
    "                f.write('\\n[%d/%d] - ptime: %.2f, rec img loss: %.9f, rec txt loss: %.9f, KL loss: %.9f, misogynous loss: %.9f, shaming loss: %.9f, stereotype loss: %.9f, objectification loss: %.9f, violence loss: %.9f\\n' % (\n",
    "                    (epoch + 1), train_epoch, per_epoch_ptime, rec_img_loss, rec_txt_loss, kl_loss, subtask_misogynous_loss, subtask_shaming_loss, subtask_stereotype_loss, subtask_objectification_loss, subtask_violence_loss))\n",
    "                rec_txt_loss = 0\n",
    "                rec_img_loss = 0\n",
    "                kl_loss = 0\n",
    "                with torch.no_grad():\n",
    "#                     test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, valid_iterator, criterion, device, \"misogynous\")\n",
    "                    test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, valid_iterator, criterion, device)\n",
    "                    f.write(f'Test subtask misogynous Loss: {test_loss[\"misogynous\"]:.3f} | Test Acc @1: {test_acc[\"misogynous\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask misogynous accuracy: {test_accuracy[\"misogynous\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask misogynous f1: {test_f1[\"misogynous\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask misogynous recall: {test_recall[\"misogynous\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask misogynous precision: {test_precision[\"misogynous\"]*100:6.2f}%\\n')\n",
    "                    \n",
    "#                     test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, trial_iterator, criterion, device, \"shaming\")\n",
    "                    f.write(f'Test subtask shaming Loss: {test_loss[\"shaming\"]:.3f} | Test Acc @1: {test_acc[\"shaming\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask shaming accuracy: {test_accuracy[\"shaming\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask shaming f1: {test_f1[\"shaming\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask shaming recall: {test_recall[\"shaming\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask shaming precision: {test_precision[\"shaming\"]*100:6.2f}%\\n')\n",
    "                    \n",
    "#                     test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, trial_iterator, criterion, device, \"stereotype\")\n",
    "                    f.write(f'Test subtask stereotype Loss: {test_loss[\"stereotype\"]:.3f} | Test Acc @1: {test_acc[\"stereotype\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask stereotype accuracy: {test_accuracy[\"stereotype\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask stereotype f1: {test_f1[\"stereotype\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask stereotype recall: {test_recall[\"stereotype\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask stereotype precision: {test_precision[\"stereotype\"]*100:6.2f}%\\n')\n",
    "                    \n",
    "#                     test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, trial_iterator, criterion, device, \"objectification\")\n",
    "                    f.write(f'Test subtask objectification Loss: {test_loss[\"objectification\"]:.3f} | Test Acc @1: {test_acc[\"objectification\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask objectification accuracy: {test_accuracy[\"objectification\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask objectification f1: {test_f1[\"objectification\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask objectification recall: {test_recall[\"objectification\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask objectification precision: {test_precision[\"objectification\"]*100:6.2f}%\\n')\n",
    "                    \n",
    "#                     test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, trial_iterator, criterion, device, \"violence\")\n",
    "                    f.write(f'Test subtask violence Loss: {test_loss[\"violence\"]:.3f} | Test Acc @1: {test_acc[\"violence\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask violence accuracy: {test_accuracy[\"violence\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask violence f1: {test_f1[\"violence\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask violence recall: {test_recall[\"violence\"]*100:6.2f}%\\n')\n",
    "                    f.write(f'Test subtask violence precision: {test_precision[\"violence\"]*100:6.2f}%\\n')\n",
    "                    \n",
    "                    acc /= num\n",
    "                    print(f'num_correct: {acc}\\n')\n",
    "                    print(f'total_num: {num}\\n')\n",
    "                    f.write(f'Training accuracy: {acc*100:6.2f}%\\n')\n",
    "                    \n",
    "                    if test_f1[\"misogynous\"]*100 >= f1_max:\n",
    "                        \n",
    "                        torch.save(vae.state_dict(), \"VAEmodel-clip-laser-bn-dropout-epoch-%d.pkl\" % (epoch+1))\n",
    "                        f.write(\"Epoch [%d/%d]: test f1 on misogynous improves, saving training results\\n\" % (epoch+1, train_epoch))\n",
    "                        f1_max = test_f1[\"misogynous\"]*100\n",
    "                    \n",
    "#                     vae.eval()\n",
    "#                     x_rec, _, _, _ = vae(inputs)\n",
    "#                     resultsample = torch.cat([inputs, x_rec]) * 0.5 + 0.5\n",
    "#                     resultsample = resultsample.cpu()\n",
    "#                     save_image(resultsample.view(-1, 3, im_size, im_size),\n",
    "#                                'results_rec_64/sample_' + str(epoch) + \"_\" + str(i) + '.png')\n",
    "#                     x_rec = vae.decode(sample1)\n",
    "#                     resultsample = x_rec * 0.5 + 0.5\n",
    "#                     resultsample = resultsample.cpu()\n",
    "#                     save_image(resultsample.view(-1, 3, im_size, im_size),\n",
    "#                                'results_gen_64/sample_' + str(epoch) + \"_\" + str(i) + '.png')\n",
    "\n",
    "        f.flush()\n",
    "\n",
    "    f.write(\"Training finish!... save training results\\n\")\n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    with torch.no_grad():\n",
    "        batch_size = y.shape[0]\n",
    "        _, top_pred = y_pred.topk(1, 1)\n",
    "        top_pred = top_pred.t()\n",
    "        correct = top_pred.eq(y.view(1, -1).expand_as(top_pred))\n",
    "        correct_1 = correct[:1].reshape(-1).float().sum(0, keepdim = True)\n",
    "        acc_1 = correct_1 / batch_size\n",
    "    \n",
    "    top_pred = top_pred.cpu().view(batch_size)\n",
    "    y = y.cpu()\n",
    "    \n",
    "#     print(y.shape)\n",
    "#     print(y)\n",
    "#     print(top_pred.shape)\n",
    "#     print(top_pred)\n",
    "    accuracy = accuracy_score(y, top_pred)\n",
    "    #print(\"accuracy: {}\".format(accuracy))\n",
    "\n",
    "    f1 = f1_score(y, top_pred)\n",
    "#     print(top_pred)\n",
    "    #print(\"f1: {}\".format(f1))\n",
    "\n",
    "    recall = recall_score(y, top_pred)\n",
    "    #print(\"recall: {}\".format(recall))\n",
    "\n",
    "    precision = precision_score(y, top_pred)\n",
    "    #print(\"precision: {}\".format(precision))\n",
    "\n",
    "    cm = confusion_matrix(y, top_pred)\n",
    "    #print(\"cm: {}\".format(cm))\n",
    "    return acc_1, accuracy, f1, recall, precision, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, device, subtask_name=\"misogynous\"):\n",
    "    \n",
    "    epoch_loss = dict()\n",
    "    epoch_loss[\"misogynous\"] = 0\n",
    "    epoch_loss[\"shaming\"] = 0\n",
    "    epoch_loss[\"stereotype\"] = 0\n",
    "    epoch_loss[\"objectification\"] = 0\n",
    "    epoch_loss[\"violence\"] = 0\n",
    "    \n",
    "    epoch_acc = dict()\n",
    "    epoch_acc[\"misogynous\"] = 0\n",
    "    epoch_acc[\"shaming\"] = 0\n",
    "    epoch_acc[\"stereotype\"] = 0\n",
    "    epoch_acc[\"objectification\"] = 0\n",
    "    epoch_acc[\"violence\"] = 0\n",
    "    \n",
    "    epoch_accuracy = dict()\n",
    "    epoch_accuracy[\"misogynous\"] = 0\n",
    "    epoch_accuracy[\"shaming\"] = 0\n",
    "    epoch_accuracy[\"stereotype\"] = 0\n",
    "    epoch_accuracy[\"objectification\"] = 0\n",
    "    epoch_accuracy[\"violence\"] = 0\n",
    "    \n",
    "    epoch_f1 = dict()\n",
    "    epoch_f1[\"misogynous\"] = 0\n",
    "    epoch_f1[\"shaming\"] = 0\n",
    "    epoch_f1[\"stereotype\"] = 0\n",
    "    epoch_f1[\"objectification\"] = 0\n",
    "    epoch_f1[\"violence\"] = 0\n",
    "    \n",
    "    epoch_recall = dict()\n",
    "    epoch_recall[\"misogynous\"] = 0\n",
    "    epoch_recall[\"shaming\"] = 0\n",
    "    epoch_recall[\"stereotype\"] = 0\n",
    "    epoch_recall[\"objectification\"] = 0\n",
    "    epoch_recall[\"violence\"] = 0\n",
    "    \n",
    "    epoch_precision = dict()\n",
    "    epoch_precision[\"misogynous\"] = 0\n",
    "    epoch_precision[\"shaming\"] = 0\n",
    "    epoch_precision[\"stereotype\"] = 0\n",
    "    epoch_precision[\"objectification\"] = 0\n",
    "    epoch_precision[\"violence\"] = 0\n",
    "    \n",
    "    epoch_cm = dict()\n",
    "    epoch_cm[\"misogynous\"] = 0\n",
    "    epoch_cm[\"shaming\"] = 0\n",
    "    epoch_cm[\"stereotype\"] = 0\n",
    "    epoch_cm[\"objectification\"] = 0\n",
    "    epoch_cm[\"violence\"] = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    name_dict = dict()\n",
    "    name_dict[\"misogynous\"] = 0\n",
    "    name_dict[\"shaming\"] = 1\n",
    "    name_dict[\"stereotype\"] = 2\n",
    "    name_dict[\"objectification\"] = 3\n",
    "    name_dict[\"violence\"] = 4\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        #for (x, y) in iterator:\n",
    "        for i, data in tqdm(enumerate(iterator, 0), desc='iterations'):\n",
    "\n",
    "            x_img = data['meme']\n",
    "            x_img = x_img.to(device)\n",
    "            \n",
    "            x_txt = data['text']\n",
    "            x_txt = x_txt.to(device)\n",
    "            \n",
    "#             txt_ids = data['text_ids']\n",
    "\n",
    "#             tokens_tensor = txt_ids['input_ids']\n",
    "#             segments_tensor = txt_ids['attention_mask']\n",
    "            \n",
    "#             tokens_tensor = tokens_tensor.to(device).squeeze(axis=1)\n",
    "#             segments_tensor = segments_tensor.to(device).squeeze(axis=1)\n",
    "#             with torch.no_grad():\n",
    "#                 x_txt = bert_pretrained(tokens_tensor, segments_tensor, output_hidden_states=True)\n",
    "#                 # last hidden state\n",
    "#                 x_txt = x_txt[1][-1]   # [bs, max_length, 768]\n",
    "#                 x_txt = x_txt[:, 0, :]   # [CLS] token embedding represent the whole sentence\n",
    "            \n",
    "            y = data['labels']\n",
    "#             y = y[:, name_dict[subtask_name]]\n",
    "\n",
    "#             _, x_img = resnet_pretrained(x_img)\n",
    "#             x_img = clip_pretrianed(x_img)\n",
    "            \n",
    "#             inputs = tokenizer(x_txt, return_tensors=\"pt\", padding=\"max_length\")   # 512\n",
    "#             x_txt = bert_pretrained(**inputs, output_hidden_states=True)\n",
    "#             # last hidden state\n",
    "#             x_txt = x_txt[1][-1]   # [bs, max_length, 768]\n",
    "#             x_txt = x_txt[:, 0, :]   # [CLS] token embedding represent the whole sentence\n",
    "            \n",
    "            x_img, x_txt = x_img.to(device), x_txt.to(device)\n",
    "            \n",
    "#             y = y.to(device)\n",
    "\n",
    "            _, _, _, _, y_pred = model(x_img, x_txt)\n",
    "            \n",
    "#             loss = criterion(y_pred[subtask_name], y)\n",
    "\n",
    "#             acc, accuracy, f1, recall, precision, cm = calculate_accuracy(y_pred[subtask_name], y)\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "#             epoch_acc += acc.item()\n",
    "#             epoch_accuracy += accuracy.item()\n",
    "#             epoch_f1 += f1.item()\n",
    "#             epoch_recall += recall.item()\n",
    "#             epoch_precision += precision.item()\n",
    "#             # epoch_cm += cm.item()\n",
    "            \n",
    "            for subtask_name, subtask_index in name_dict.items():\n",
    "                subtask_y = y[:, subtask_index]\n",
    "                subtask_y = subtask_y.to(device)\n",
    "                loss = criterion(y_pred[subtask_name], subtask_y)\n",
    "                acc, accuracy, f1, recall, precision, cm = calculate_accuracy(y_pred[subtask_name], subtask_y)\n",
    "                \n",
    "                epoch_loss[subtask_name] += loss.item()\n",
    "                epoch_acc[subtask_name] += acc.item()\n",
    "                epoch_accuracy[subtask_name] += accuracy.item()\n",
    "                epoch_f1[subtask_name] += f1.item()\n",
    "                epoch_recall[subtask_name] += recall.item()\n",
    "                epoch_precision[subtask_name] += precision.item()\n",
    "                # epoch_cm += cm.item()\n",
    "                \n",
    "                \n",
    "    for subtask_name, subtask_index in name_dict.items():\n",
    "        epoch_loss[subtask_name] /= len(iterator)\n",
    "        epoch_acc[subtask_name] /= len(iterator)\n",
    "        epoch_accuracy[subtask_name] /= len(iterator)\n",
    "        epoch_f1[subtask_name] /= len(iterator)\n",
    "        epoch_recall[subtask_name] /= len(iterator)\n",
    "        epoch_precision[subtask_name] /= len(iterator)\n",
    "                \n",
    "        \n",
    "#     epoch_loss /= len(iterator)\n",
    "#     epoch_acc /= len(iterator)\n",
    "#     epoch_accuracy /= len(iterator)\n",
    "#     epoch_f1 /= len(iterator)\n",
    "#     epoch_recall /= len(iterator)\n",
    "#     epoch_precision /= len(iterator)\n",
    "#     # epoch_cm /= len(iterator)\n",
    "        \n",
    "    return epoch_loss, epoch_acc, epoch_accuracy, epoch_f1, epoch_recall, epoch_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5351ef14e6e54c2f8668c67fa0c2f3e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='iterations', max=1.0, style=ProgressSty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee40f56be5c842e0accff6e674e19522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='iterations', max=1.0, style=ProgressSty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_correct: 0.7284444444444444\n",
      "\n",
      "total_num: 9000\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8b71a5069446f9b2d718e32584a450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='iterations', max=1.0, style=ProgressSty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vae = main()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, trial_iterator, criterion, device, \"misogynous\")\n",
    "\n",
    "f.write(f'Trial subtask misogynous Loss: {test_loss:.3f} | Test Acc @1: {test_acc*100:6.2f}%\\n')\n",
    "f.write(f'Trial subtask misogynous accuracy: {test_accuracy*100:6.2f}%\\n')\n",
    "f.write(f'Trial subtask misogynous f1: {test_f1*100:6.2f}%\\n')\n",
    "f.write(f'Trial subtask misogynous recall: {test_recall*100:6.2f}%\\n')\n",
    "f.write(f'Trial subtask misogynous precision: {test_precision*100:6.2f}%\\n')\n",
    "\n",
    "test_loss, test_acc, test_accuracy, test_f1, test_recall, test_precision = evaluate(vae, trial_iterator, criterion, device, \"shaming\")\n",
    "\n",
    "f.write(f'Trial subtask shaming Loss: {test_loss:.3f} | Test Acc @1: {test_acc*100:6.2f}%\\n')\n",
    "f.write(f'Trial subtask shaming accuracy: {test_accuracy*100:6.2f}%\\n')\n",
    "f.write(f'Trial subtask shaming f1: {test_f1*100:6.2f}%\\n')\n",
    "f.write(f'Trial subtask shaming recall: {test_recall*100:6.2f}%\\n')\n",
    "f.write(f'Trial subtask shaming precision: {test_precision*100:6.2f}%\\n')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), \"VAEmodel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "print('123139013')\n",
    "with open('output.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, iterator, device):\n",
    "    \n",
    "    name_dict = dict()\n",
    "    name_dict[\"misogynous\"] = 0\n",
    "    name_dict[\"shaming\"] = 1\n",
    "    name_dict[\"stereotype\"] = 2\n",
    "    name_dict[\"objectification\"] = 3\n",
    "    name_dict[\"violence\"] = 4\n",
    "    \n",
    "    y_test = dict()\n",
    "    y_test[\"misogynous\"] = []\n",
    "    y_test[\"shaming\"] = []\n",
    "    y_test[\"stereotype\"] = []\n",
    "    y_test[\"objectification\"] = []\n",
    "    y_test[\"violence\"] = []\n",
    "    \n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, data in tqdm(enumerate(iterator, 0), desc='iterations'):\n",
    "\n",
    "            x_img = data['meme']\n",
    "            x_img = x_img.to(device)\n",
    "            \n",
    "            x_txt = data['text']\n",
    "            \n",
    "            x_img, x_txt = x_img.to(device), x_txt.to(device)\n",
    "\n",
    "            _, _, _, _, y_pred = model(x_img, x_txt)\n",
    "            \n",
    "            \n",
    "            for subtask_name, subtask_index in name_dict.items():\n",
    "                subtask_y = y_pred[subtask_name].cpu()\n",
    "                for dp in subtask_y:\n",
    "                    if dp[0] >= dp[1]:\n",
    "                        y_test[subtask_name].append(0)\n",
    "                    else:\n",
    "                        y_test[subtask_name].append(1)\n",
    "        \n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_VAE = VAE(512)\n",
    "best_VAE.load_state_dict(torch.load(\"VAEmodel-clip-laser-bn-dropout-epoch-6.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c09fef636dd46349eb8ba54cc1773e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='iterations', max=1.0, style=ProgressSty"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = test(best_VAE, test_iterator, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test[\"misogynous\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = test_df.copy()\n",
    "\n",
    "prediction_df[\"misogynous\"] = y_test[\"misogynous\"]\n",
    "prediction_df[\"shaming\"] = y_test[\"shaming\"]\n",
    "prediction_df[\"stereotype\"] = y_test[\"stereotype\"]\n",
    "prediction_df[\"objectification\"] = y_test[\"objectification\"]\n",
    "prediction_df[\"violence\"] = y_test[\"violence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>misogynous</th>\n",
       "      <th>shaming</th>\n",
       "      <th>stereotype</th>\n",
       "      <th>objectification</th>\n",
       "      <th>violence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15236.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15805.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16254.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16191.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15952.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>15591.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>15049.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>15363.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>15199.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>15853.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_name  misogynous  shaming  stereotype  objectification  violence\n",
       "0    15236.jpg           1        0           0                0         0\n",
       "1    15805.jpg           1        0           1                1         0\n",
       "2    16254.jpg           1        0           0                0         0\n",
       "3    16191.jpg           1        0           0                0         0\n",
       "4    15952.jpg           1        0           0                0         0\n",
       "..         ...         ...      ...         ...              ...       ...\n",
       "995  15591.jpg           1        0           1                1         0\n",
       "996  15049.jpg           1        0           0                0         0\n",
       "997  15363.jpg           1        0           0                0         0\n",
       "998  15199.jpg           1        0           1                1         0\n",
       "999  15853.jpg           0        0           0                0         0\n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df = prediction_df.drop('Text Transcription', 1)\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtask A and subtask B\n",
    "with open(\"answer.txt\", \"w\") as f:\n",
    "    for i, row in prediction_df.iterrows():\n",
    "        f.write(row[\"file_name\"] + '\\t' + str(row[\"misogynous\"]) + '\\t' + str(row[\"shaming\"]) + '\\t' + str(row[\"stereotype\"]) + '\\t' + str(row[\"objectification\"]) + '\\t' + str(row[\"violence\"]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: answer.txt (deflated 84%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip submission.zip answer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtask A and subtask B\n",
    "results_7 = []\n",
    "with open(\"answer7.txt\", \"r\") as f_7:\n",
    "    lines = f_7.readlines()\n",
    "    for line in lines:\n",
    "        items = line.strip().split('\\t')\n",
    "        results_7.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtask A and subtask B\n",
    "results_18 = []\n",
    "with open(\"answer18.txt\", \"r\") as f_18:\n",
    "    lines = f_18.readlines()\n",
    "    for line in lines:\n",
    "        items = line.strip().split('\\t')\n",
    "        results_18.append(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"answer.txt\", \"w\") as f:\n",
    "    for i in range(len(results_18)):\n",
    "        f.write(results_18[i][0] + '\\t' + results_18[i][1] + '\\t' + results_7[i][2] + '\\t' + results_7[i][3] + '\\t' + results_7[i][4] + '\\t' + results_7[i][5] + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: answer.txt (deflated 84%)\r\n"
     ]
    }
   ],
   "source": [
    "!zip submission.zip answer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in tqdm(enumerate(trial_iterator, 0), desc='iterations'):\n",
    "        #for x in batches:\n",
    "            \n",
    "        inputs = data['meme']\n",
    "        classes = data['labels']\n",
    "        print(classes[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo fuser -v /dev/nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo kill -9 4103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
